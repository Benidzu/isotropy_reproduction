{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import semcor\r\n",
    "from IPython.display import clear_output\r\n",
    "from functions import *\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import *\r\n",
    "from transformers.tokenization_utils import TextInputPair\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "import tensorflow as tf\r\n",
    "import pickle\r\n",
    "import scipy as sc\r\n",
    "import math as mt\r\n",
    "from joblib import dump, load\r\n",
    "from sklearn.neighbors import KNeighborsTransformer\r\n",
    "from collections import defaultdict\r\n",
    "from scipy.spatial.distance import pdist, cdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\r\n",
    "casing = \"bert-base-uncased\" \r\n",
    "tokenizer = BertTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "\r\n",
    "config = BertConfig(dropout=0.2, attention_dropout=0.2 ) #hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2\r\n",
    "config.output_hidden_states = False # if true outputs all layers\r\n",
    "\r\n",
    "model = TFBertModel.from_pretrained(casing, config = config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# BERT\r\n",
    "n_cluster = 27 # Number of clusters to use\r\n",
    "n_pc = 12 # Number of main principal components to drop for local method\r\n",
    "n_pc_global = 15 # Number of main principal components to drop for global method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2\r\n",
    "casing = \"gpt2\" \r\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = GPT2Config()\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFGPT2Model.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# GPT2\r\n",
    "n_cluster = 10\r\n",
    "n_pc = 30\r\n",
    "n_pc_global = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa\r\n",
    "casing = \"roberta-base\"\r\n",
    "tokenizer = RobertaTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = RobertaConfig.from_pretrained(casing)\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFRobertaModel.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# RoBERTa\r\n",
    "n_cluster = 27\r\n",
    "n_pc = 12\r\n",
    "n_pc_global = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"semcor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = semcor.tagged_sents(tag=\"sem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get verbs representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata - get all verbs from the dataset\r\n",
    "verbs = [] # sentix, wordix, label, word\r\n",
    "sentix = 0\r\n",
    "for sent in sents:\r\n",
    "    wordix = 0\r\n",
    "    for word in sent:\r\n",
    "        if type(word) == nltk.tree.Tree:\r\n",
    "            if word.label() and type(word.label()) == nltk.corpus.reader.wordnet.Lemma:\r\n",
    "                if word.label().synset() and word.label().synset().pos() == \"v\":\r\n",
    "                    verbs.append((sentix,wordix,word.label(),word[0]))\r\n",
    "        wordix+=1\r\n",
    "    sentix+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of verbs we are interested in - provided by authors\r\n",
    "trueverbs = [\r\n",
    "    'say', 'said', 'have', 'had', 'win', 'won', 'study', 'studied', 'find', 'found', 'hold', 'held', 'make', 'made', 'tell', 'told', 'seek', 'sought', 'see', 'saw', 'get', 'got', 'shoot', 'shot', 'go', 'went', 'lead', 'led', 'leave', 'left', 'deny', 'denied', 'send', 'sent', 'keep', 'kept', 'lose', 'lost', 'feel', 'felt', 'spend', 'spent', 'draw', 'drew', 'throw', 'threw', 'try', 'tried', 'pay', 'paid', 'break', 'broke', 'come', 'came', 'run', 'ran', 'think', 'thought', 'carry', 'carried', 'catch', 'caught', 'lie', 'lay', 'fall', 'fell', 'write', 'wrote', 'know', 'knew', 'stand', 'stood', 'teach', 'taught', 'fight', 'fought', 'rise', 'rose', 'speak', 'spoken', 'choose', 'chosen', 'forget', 'forgotten', 'strike', 'struck', 'meet', 'met', 'build', 'built', 'apply', 'applied', 'sit', 'sat', 'sell', 'sold', 'buy', 'bought', 'feed', 'fed', 'ride', 'rode', 'drive', 'drove', 'wear', 'wore'\r\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out verbs we don't care about\r\n",
    "filtered_verbs = list(filter(lambda x: x[3] in trueverbs,verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of only the sentences that contain the verbs we are interested in\r\n",
    "sents_to_parse = set()\r\n",
    "for vb in filtered_verbs:\r\n",
    "    sents_to_parse = sents_to_parse.union([vb[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "11838"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents_to_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(sorted(sents_to_parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps to translate actual sentence index -> filtered sentence index and vice versa\r\n",
    "translation_dict = dict(zip(idxs,range(len(idxs))))\r\n",
    "translation_dict_inv = dict(zip(range(len(idxs)),idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sentences we care about\r\n",
    "finalsents = []\r\n",
    "i = 0\r\n",
    "for sent in semcor.sents():\r\n",
    "    if i in idxs:\r\n",
    "        finalsents.append(sent)\r\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\r\n",
    "for i in range(len(finalsents)):\r\n",
    "    for j in range(len(finalsents[i])):\r\n",
    "        finalsents[i][j] = finalsents[i][j].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\r\n",
    "ids = []\r\n",
    "for sent in finalsents:\r\n",
    "    ids.append(tokenizer.convert_tokens_to_ids(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send through model... Run commented lines if no embeddings saved\r\n",
    "with open('tense_roberta.pkl', 'rb') as f:\r\n",
    "    reps = pickle.load(f)\r\n",
    "# reps = []\r\n",
    "# for i in range(len(ids)):\r\n",
    "#     reps.append(model(np.asarray([ids[i]], dtype=\"int32\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('gpt2_tense.pkl', 'wb') as f:\r\n",
    "#     pickle.dump(reps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure to keep our verbs embeddings organized based on sense and tense\r\n",
    "list_of_dicts = [] # will keep dicts of verbs in order of trueverbs, {synset: [rep]}\r\n",
    "# for each verb, (for present and past form), collect their CWRs from the model outputs, store them in dicts\r\n",
    "for i in range(len(trueverbs)//2):\r\n",
    "    present_verb = trueverbs[i*2]\r\n",
    "    past_verb = trueverbs[i*2+1]\r\n",
    "    present_dict = defaultdict(list)\r\n",
    "    past_dict = defaultdict(list)\r\n",
    "    # go over all filtered verbs, fetch their CWRs... \r\n",
    "    for j in range(len(filtered_verbs)):\r\n",
    "        # present\r\n",
    "        if filtered_verbs[j][3] == present_verb:\r\n",
    "            sent_ix = translation_dict[filtered_verbs[j][0]]\r\n",
    "            wrd_ix = filtered_verbs[j][1]\r\n",
    "            present_dict[filtered_verbs[j][2].synset().name()].append(reps[sent_ix][0][wrd_ix].numpy())\r\n",
    "        # past\r\n",
    "        elif filtered_verbs[j][3] == past_verb:\r\n",
    "            sent_ix = translation_dict[filtered_verbs[j][0]]\r\n",
    "            wrd_ix = filtered_verbs[j][1]\r\n",
    "            past_dict[filtered_verbs[j][2].synset().name()].append(reps[sent_ix][0][wrd_ix].numpy())\r\n",
    "    \r\n",
    "    list_of_dicts.append(present_dict)\r\n",
    "    list_of_dicts.append(past_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Metrics defined in paper\r\n",
    "\r\n",
    "def st_sm(list_of_dicts):\r\n",
    "    \"\"\"\r\n",
    "        From list of dicts, get average distance between verb representations of same tense and same meaning \r\n",
    "    \"\"\"\r\n",
    "    # go over all verbs\r\n",
    "    means = []\r\n",
    "    for i in range(len(list_of_dicts)//2):\r\n",
    "        present_verb_dict = list_of_dicts[2*i]\r\n",
    "        past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "        # go over all of this verbs present tense synsets\r\n",
    "        for synset in present_verb_dict.keys():\r\n",
    "            verb_representations = present_verb_dict[synset]\r\n",
    "            if len(verb_representations) > 1:\r\n",
    "                means+= list(pdist(np.array(verb_representations), 'euclidean'))\r\n",
    "\r\n",
    "        # go over all of this verbs past tense synsets\r\n",
    "        for synset in past_verb_dict.keys():\r\n",
    "            verb_representations = past_verb_dict[synset]\r\n",
    "            if len(verb_representations) > 1:\r\n",
    "                means+= list(pdist(np.array(verb_representations), 'euclidean'))\r\n",
    "                #means.append(pdist(np.array(verb_representations), 'euclidean').mean())\r\n",
    "\r\n",
    "    return np.mean(means)\r\n",
    "\r\n",
    "def st_dm(list_of_dicts):\r\n",
    "    \"\"\"\r\n",
    "        From list of dicts, get average distance between verb representations of same tense and different meaning \r\n",
    "    \"\"\"\r\n",
    "    # go over all verbs\r\n",
    "    means = []\r\n",
    "    for i in range(len(list_of_dicts)//2):\r\n",
    "        present_verb_dict = list_of_dicts[2*i]\r\n",
    "        past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "        # go over all of this verbs present tense synsets\r\n",
    "        for synset in present_verb_dict.keys():\r\n",
    "            verb_representations = present_verb_dict[synset]\r\n",
    "            # get all other synsets represenattions \r\n",
    "            other_synsets = [x for x in present_verb_dict.keys() if x!= synset]\r\n",
    "            other_syn_reps = []\r\n",
    "            for ss in other_synsets:\r\n",
    "                other_syn_reps+= present_verb_dict[ss]\r\n",
    "            other_syn_reps = np.array(other_syn_reps)\r\n",
    "\r\n",
    "            \r\n",
    "            if len(other_syn_reps.shape) == 2 and len(verb_representations) > 0:\r\n",
    "                means.append(cdist(np.array(verb_representations), other_syn_reps, 'euclidean'))\r\n",
    "            \r\n",
    "\r\n",
    "        # go over all of this verbs past tense synsets\r\n",
    "        for synset in past_verb_dict.keys():\r\n",
    "            verb_representations = past_verb_dict[synset]\r\n",
    "            # get all other synsets represenattions \r\n",
    "            other_synsets = [x for x in past_verb_dict.keys() if x!= synset]\r\n",
    "            other_syn_reps = []\r\n",
    "            for ss in other_synsets:\r\n",
    "                other_syn_reps+= past_verb_dict[ss]\r\n",
    "            other_syn_reps = np.array(other_syn_reps)\r\n",
    "\r\n",
    "            if len(other_syn_reps.shape) == 2 and len(verb_representations) > 0:\r\n",
    "                means.append(cdist(np.array(verb_representations), other_syn_reps, 'euclidean'))\r\n",
    "                #means+= list(pdist(np.array(verb_representations), 'euclidean'))\r\n",
    "\r\n",
    "    finalels = []\r\n",
    "    for arr in means:\r\n",
    "        finalels += arr.flatten().tolist()\r\n",
    "    return np.mean(finalels)\r\n",
    "\r\n",
    "def dt_sm(list_of_dicts):\r\n",
    "    \"\"\"\r\n",
    "        From list of dicts, get average distance between verb representations of different tense and same meaning \r\n",
    "    \"\"\"\r\n",
    "    # go over all verbs\r\n",
    "    means = []\r\n",
    "    for i in range(len(list_of_dicts)//2):\r\n",
    "        present_verb_dict = list_of_dicts[2*i]\r\n",
    "        past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "        # look at intersection of synsets\r\n",
    "        synsets_intersect = list(set(present_verb_dict.keys()).intersection(past_verb_dict.keys()))\r\n",
    "\r\n",
    "        # go over all of this verbs filtered synsets\r\n",
    "        for synset in synsets_intersect:\r\n",
    "            verb_representations_pres = present_verb_dict[synset]\r\n",
    "            verb_representation_past = past_verb_dict[synset]\r\n",
    "            \r\n",
    "            if  len(verb_representation_past) > 0 and len(verb_representations_pres) > 0:\r\n",
    "                means.append(cdist(np.array(verb_representations_pres), np.array(verb_representation_past), 'euclidean'))\r\n",
    "            \r\n",
    "    finalels = []\r\n",
    "    for arr in means:\r\n",
    "        finalels += arr.flatten().tolist()\r\n",
    "    return np.mean(finalels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_sm(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dm(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_sm(list_of_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all representations to measure their isotropy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reps = []\r\n",
    "for ix in range(len(list_of_dicts)):\r\n",
    "    for key in list_of_dicts[ix].keys():\r\n",
    "        all_reps += list_of_dicts[ix][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "14955"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.4068748e-05"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isotropy(all_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat process with cluster-based method imrovement (removing dominant components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isotropy calculated based on enhancing all verbs CWRs at once\r\n",
    "clstrd_reps = cluster_based(np.array(all_reps), 1, n_pc, emb_len)\r\n",
    "isotropy(clstrd_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better approach : cluster & enhance based on each verb seperately\r\n",
    "new_dicts = []\r\n",
    "\r\n",
    "for i in range(len(list_of_dicts)//2): #len(list_of_dicts)//2\r\n",
    "    present_verb_dict = list_of_dicts[2*i]\r\n",
    "    past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "    combined_reps = [] \r\n",
    "    for key in present_verb_dict.keys():\r\n",
    "        combined_reps += present_verb_dict[key]\r\n",
    "    for key in past_verb_dict.keys():\r\n",
    "        combined_reps += past_verb_dict[key]\r\n",
    "\r\n",
    "\r\n",
    "    cb_reps = cluster_based(np.array(combined_reps), 1, n_pc, 768)\r\n",
    "\r\n",
    "    present_dict_new = defaultdict(list)\r\n",
    "    past_dict_new = defaultdict(list)\r\n",
    "\r\n",
    "    u = 0\r\n",
    "    for key in present_verb_dict.keys():\r\n",
    "        present_dict_new[key] = []\r\n",
    "        for ix in range(len(present_verb_dict[key])):\r\n",
    "            present_dict_new[key].append(cb_reps[u])\r\n",
    "            u+=1\r\n",
    "    for key in past_verb_dict.keys():\r\n",
    "        past_dict_new[key] = []\r\n",
    "        for ix in range(len(past_verb_dict[key])):\r\n",
    "            past_dict_new[key].append(cb_reps[u])\r\n",
    "            u+=1\r\n",
    "            \r\n",
    "\r\n",
    "    new_dicts.append(present_dict_new)\r\n",
    "    new_dicts.append(past_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.102383363457763"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sm(new_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.487159470883429"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_dm(new_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.463559610829546"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_sm(new_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reps_new = []\r\n",
    "for ix in range(len(new_dicts)):\r\n",
    "    for key in new_dicts[ix].keys():\r\n",
    "        all_reps_new += new_dicts[ix][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotropy(all_reps_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit",
   "name": "python3811jvsc74a57bd0556d0f6bb3e19b5350a50c9b037347830b2ef0c6b7dbb2b89c6068584bda62c6"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}