{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\r\n",
    "from functions import *\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import *\r\n",
    "import tensorflow as tf\r\n",
    "import pickle\r\n",
    "import scipy as sc\r\n",
    "import math as mt\r\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = pd.read_csv('sts-dev.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "dfd = dfd[np.logical_not(pd.isna(dfd[\"sentence2\"]))]\r\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\r\n",
    "casing = \"bert-base-uncased\" \r\n",
    "tokenizer = BertTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "\r\n",
    "config = BertConfig(dropout=0.2, attention_dropout=0.2 ) #hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFBertModel.from_pretrained(casing, config = config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# BERT\r\n",
    "n_cluster = 27 # Number of clusters to use\r\n",
    "n_pc = 12 # Number of main principal components to drop for local method\r\n",
    "n_pc_global = 15 # Number of main principal components to drop for global method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2\r\n",
    "casing = \"gpt2\" \r\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = GPT2Config()\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFGPT2Model.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# GPT2\r\n",
    "n_cluster = 10\r\n",
    "n_pc = 30\r\n",
    "n_pc_global = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa\r\n",
    "casing = \"roberta-base\"\r\n",
    "tokenizer = RobertaTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = RobertaConfig.from_pretrained(casing)\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFRobertaModel.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# RoBERTa\r\n",
    "n_cluster = 27\r\n",
    "n_pc = 12\r\n",
    "n_pc_global = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get CWRs at different layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representations_all_layers(data_, tokenizer, model, emb_length):\r\n",
    "    sentences = []\r\n",
    "    for i in range(len(data_)):\r\n",
    "        print(i)\r\n",
    "        # First sentence\r\n",
    "        inputs = tokenizer.encode(\r\n",
    "            data_['sentence1'].iloc[i], add_special_tokens=True)\r\n",
    "        inputs = np.asarray(inputs, dtype='int32').reshape((1, -1))\r\n",
    "\r\n",
    "        # getting the representation of the last layer\r\n",
    "        output = model(inputs)[2]\r\n",
    "        #print(output)\r\n",
    "        output = np.asarray(output).reshape((13,-1, emb_length))\r\n",
    "        \r\n",
    "        # Removing CLS and SEP tokens\r\n",
    "        idx = [0, len(output[0])-1]\r\n",
    "        output = np.delete(output, idx, axis=1)\r\n",
    "        #output = np.asarray(output).reshape((-1, emb_length))\r\n",
    "\r\n",
    "        sentences.append(output)\r\n",
    "\r\n",
    "        # Second sentence\r\n",
    "        inputs = tokenizer.encode(\r\n",
    "            data_['sentence2'].iloc[i], add_special_tokens=True)\r\n",
    "        inputs = np.asarray(inputs, dtype='int32').reshape((1, -1))\r\n",
    "\r\n",
    "        output = model(inputs)[2]\r\n",
    "        output = np.asarray(output).reshape((13,-1, emb_length))\r\n",
    "\r\n",
    "        # Removing CLS and SEP tokens\r\n",
    "        idx = [0, len(output[0])-1]\r\n",
    "        output = np.delete(output, idx, axis=1)\r\n",
    "        # output = np.asarray(output).reshape((-1, emb_length))\r\n",
    "\r\n",
    "        sentences.append(output)\r\n",
    "        if i % 10 == 0:\r\n",
    "           clear_output()\r\n",
    "\r\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsAtLayer(sentences, layer):\r\n",
    "    \"\"\" Get words (tokens) representations in a list at a layer by removing the sentences axis. \"\"\"\r\n",
    "    words = []\r\n",
    "    for i in range(len(sentences)):\r\n",
    "        for j in range(len(sentences[i][0])):\r\n",
    "            words.append(sentences[i][layer][j])\r\n",
    "\r\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441\n",
      "1442\n",
      "1443\n"
     ]
    }
   ],
   "source": [
    "reps = get_representations_all_layers(dfd, tokenizer, model, emb_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008965204001531902\n",
      "2.4606133484093245e-07\n",
      "8.581332820159886e-10\n",
      "4.210619442620398e-09\n",
      "5.377646965360977e-12\n",
      "4.853519784312229e-10\n",
      "3.133070468963168e-10\n",
      "1.317334163571853e-10\n",
      "1.414390580381122e-10\n",
      "1.3529625036818678e-10\n",
      "6.511797473015736e-11\n",
      "1.4053177125577774e-10\n",
      "2.6920787864719667e-06\n"
     ]
    }
   ],
   "source": [
    "# Compute isotropy at each layer\r\n",
    "for lay in range(13):\r\n",
    "    wordsatLayer = getWordsAtLayer(reps, lay)\r\n",
    "    print(isotropy(np.asarray(wordsatLayer, dtype=np.float64)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit",
   "name": "python3811jvsc74a57bd0556d0f6bb3e19b5350a50c9b037347830b2ef0c6b7dbb2b89c6068584bda62c6"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}