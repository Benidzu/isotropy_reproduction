{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\r\n",
    "from functions import *\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import *\r\n",
    "import tensorflow as tf\r\n",
    "import pickle\r\n",
    "import scipy as sc\r\n",
    "import math as mt\r\n",
    "import random\r\n",
    "# from scipy import cluster as clst\r\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "# from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfd = pd.read_csv('sts-dev.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "#STS-B\r\n",
    "dfd = pd.read_csv('sts-test.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "#STS-2012\r\n",
    "#dfd = pd.read_csv('../data/sts-2012-test/STS.input.ALL.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "#STS-2013\r\n",
    "#dfd = pd.read_csv('../data/sts-2013-test/STS.input.ALL.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "#STS-2014\r\n",
    "#dfd = pd.read_csv('../data/sts-2014-test/STS.input.ALL.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "#STS-2015\r\n",
    "#dfd = pd.read_csv('../data/sts-2015-test/STS.input.ALL.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "#dfd = dfd[dfd[\"corr\"] != -1]\r\n",
    "#STS-2016\r\n",
    "# dfd = pd.read_csv('../data/sts-2016-test/STS2016.input.ALL.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "# dfd = dfd[dfd[\"corr\"] != -1]\r\n",
    "#SICK-R\r\n",
    "#dfd = pd.read_csv('../data/sick-r-test/SICK.csv', delimiter='\\t' , error_bad_lines=False)\r\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>subtype</th>\n      <th>year</th>\n      <th>num</th>\n      <th>corr</th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>main-captions</td>\n      <td>MSRvid</td>\n      <td>2012test</td>\n      <td>24</td>\n      <td>2.5</td>\n      <td>A girl is styling her hair.</td>\n      <td>A girl is brushing her hair.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>main-captions</td>\n      <td>MSRvid</td>\n      <td>2012test</td>\n      <td>33</td>\n      <td>3.6</td>\n      <td>A group of men play soccer on the beach.</td>\n      <td>A group of boys are playing soccer on the beach.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>main-captions</td>\n      <td>MSRvid</td>\n      <td>2012test</td>\n      <td>45</td>\n      <td>5.0</td>\n      <td>One woman is measuring another woman's ankle.</td>\n      <td>A woman measures another woman's ankle.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>main-captions</td>\n      <td>MSRvid</td>\n      <td>2012test</td>\n      <td>63</td>\n      <td>4.2</td>\n      <td>A man is cutting up a cucumber.</td>\n      <td>A man is slicing a cucumber.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>main-captions</td>\n      <td>MSRvid</td>\n      <td>2012test</td>\n      <td>66</td>\n      <td>1.5</td>\n      <td>A man is playing a harp.</td>\n      <td>A man is playing a keyboard.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1090</th>\n      <td>main-news</td>\n      <td>headlines</td>\n      <td>2015</td>\n      <td>1438</td>\n      <td>0.4</td>\n      <td>US, China fail to paper over cracks in ties</td>\n      <td>China: Relief in focus as hope for missing fades</td>\n    </tr>\n    <tr>\n      <th>1091</th>\n      <td>main-news</td>\n      <td>headlines</td>\n      <td>2015</td>\n      <td>1454</td>\n      <td>1.4</td>\n      <td>World Cup live: France 0-0 Germany</td>\n      <td>World Cup live: Germany 0-0 Ghana</td>\n    </tr>\n    <tr>\n      <th>1092</th>\n      <td>main-news</td>\n      <td>headlines</td>\n      <td>2015</td>\n      <td>1456</td>\n      <td>4.8</td>\n      <td>Tokyo to host 2020 Games</td>\n      <td>Tokyo wins race to host 2020 Olympic Games</td>\n    </tr>\n    <tr>\n      <th>1093</th>\n      <td>main-news</td>\n      <td>headlines</td>\n      <td>2015</td>\n      <td>1463</td>\n      <td>4.4</td>\n      <td>France warns of extremists benefiting from Egy...</td>\n      <td>France fears extremists will benefit from Egyp...</td>\n    </tr>\n    <tr>\n      <th>1094</th>\n      <td>main-news</td>\n      <td>headlines</td>\n      <td>2015</td>\n      <td>1482</td>\n      <td>5.0</td>\n      <td>British teenager killed in fall from Magaluf h...</td>\n      <td>British teenager killed in Magaluf hotel fall</td>\n    </tr>\n  </tbody>\n</table>\n<p>1095 rows Ã— 7 columns</p>\n</div>",
      "text/plain": "               type    subtype      year   num  corr  \\\n0     main-captions     MSRvid  2012test    24   2.5   \n1     main-captions     MSRvid  2012test    33   3.6   \n2     main-captions     MSRvid  2012test    45   5.0   \n3     main-captions     MSRvid  2012test    63   4.2   \n4     main-captions     MSRvid  2012test    66   1.5   \n...             ...        ...       ...   ...   ...   \n1090      main-news  headlines      2015  1438   0.4   \n1091      main-news  headlines      2015  1454   1.4   \n1092      main-news  headlines      2015  1456   4.8   \n1093      main-news  headlines      2015  1463   4.4   \n1094      main-news  headlines      2015  1482   5.0   \n\n                                              sentence1  \\\n0                           A girl is styling her hair.   \n1              A group of men play soccer on the beach.   \n2         One woman is measuring another woman's ankle.   \n3                       A man is cutting up a cucumber.   \n4                              A man is playing a harp.   \n...                                                 ...   \n1090        US, China fail to paper over cracks in ties   \n1091                 World Cup live: France 0-0 Germany   \n1092                           Tokyo to host 2020 Games   \n1093  France warns of extremists benefiting from Egy...   \n1094  British teenager killed in fall from Magaluf h...   \n\n                                              sentence2  \n0                          A girl is brushing her hair.  \n1      A group of boys are playing soccer on the beach.  \n2               A woman measures another woman's ankle.  \n3                          A man is slicing a cucumber.  \n4                          A man is playing a keyboard.  \n...                                                 ...  \n1090   China: Relief in focus as hope for missing fades  \n1091                  World Cup live: Germany 0-0 Ghana  \n1092         Tokyo wins race to host 2020 Olympic Games  \n1093  France fears extremists will benefit from Egyp...  \n1094      British teenager killed in Magaluf hotel fall  \n\n[1095 rows x 7 columns]"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1095"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for errors\r\n",
    "dfd = dfd[np.logical_not(pd.isna(dfd[\"sentence2\"]))]\r\n",
    "len(dfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models - choose and run one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\r\n",
    "casing = \"bert-base-uncased\" \r\n",
    "tokenizer = BertTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "\r\n",
    "config = BertConfig(dropout=0.2, attention_dropout=0.2 ) #hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFBertModel.from_pretrained(casing, config = config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# BERT\r\n",
    "n_cluster = 27 # Number of clusters to use\r\n",
    "n_pc = 12 # Number of main principal components to drop for local method\r\n",
    "n_pc_global = 15 # Number of main principal components to drop for global method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2\r\n",
    "casing = \"gpt2\" \r\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = GPT2Config()\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFGPT2Model.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# GPT2\r\n",
    "n_cluster = 10\r\n",
    "n_pc = 30\r\n",
    "n_pc_global = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa\r\n",
    "casing = \"roberta-base\"\r\n",
    "tokenizer = RobertaTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = RobertaConfig.from_pretrained(casing)\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFRobertaModel.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# RoBERTa\r\n",
    "n_cluster = 27\r\n",
    "n_pc = 12\r\n",
    "n_pc_global = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sentences and words representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence representations BERT/GPT2/RoBERTa\r\n",
    "#sentences = get_representations(dfd, tokenizer, model, emb_len)\r\n",
    "sentences = np.load(\"roberta-stsb-test.npy\", allow_pickle=True).tolist()\r\n",
    "words = getWords(sentences)\r\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1095"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER\r\n",
    "np.save(\"gpt2-sick-test.npy\", np.asarray(sentences, dtype=object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run multiple experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [ \"sts-test.csv\", \"../data/sts-2015-test/STS.input.ALL.csv\", \"../data/sts-2014-test/STS.input.ALL.csv\", \"../data/sts-2013-test/STS.input.ALL.csv\", \"../data/sts-2012-test/STS.input.ALL.csv\"]\r\n",
    "# npynames = [\"gpt2-stsb-test.npy\", \"gpt2-sts2015-test.npy\", \"gpt2-sts2014-test.npy\", \"gpt2-sts2013-test.npy\", \"gpt2-sts2012-test.npy\" ]\r\n",
    "\r\n",
    "# for i in range(len(datasets)):\r\n",
    "#     dsName = datasets[i]\r\n",
    "#     npyName = npynames[i]\r\n",
    "#     dfd = pd.read_csv(dsName, delimiter='\\t' , error_bad_lines=False)\r\n",
    "#     dfd = dfd[dfd[\"corr\"] != -1]\r\n",
    "#     dfd = dfd[np.logical_not(pd.isna(dfd[\"sentence2\"]))]\r\n",
    "#     sentences = get_representations(dfd, tokenizer, model, emb_len)\r\n",
    "#     np.save(npyName, np.asarray(sentences, dtype=object))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get representations via baseline, global method, local (isotropy enhancing) method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE\r\n",
    "baseline_sentence_rep = mean_pooling(words, sentences)\r\n",
    "baseline_score = similarity(baseline_sentence_rep, emb_len)\r\n",
    "\r\n",
    "# GLOBAL METHOD\r\n",
    "global_representations = global_method(np.asarray(words), n_pc_global, emb_len)\r\n",
    "global_sentence_rep = mean_pooling(global_representations, sentences)\r\n",
    "global_score = similarity(global_sentence_rep, emb_len)\r\n",
    "\r\n",
    "# LOCAL METHOD\r\n",
    "#isotropic_representations = cluster_based(np.asarray(words), n_cluster, n_pc, emb_len)\r\n",
    "# calculating sentence representations based on mean word (isotropised) representations\r\n",
    "#sentence_rep = mean_pooling(isotropic_representations, sentences)\r\n",
    "#local_score = similarity(sentence_rep, emb_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOOTSTRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  4 , means and errors:\n",
      "local: 72.2106085872343 +- 1.1794197221874692\n"
     ]
    }
   ],
   "source": [
    "means_global = []\r\n",
    "means_local = []\r\n",
    "means_baseline = []\r\n",
    "all_scores_local = []\r\n",
    "stds_local = []\r\n",
    "stds_global = []\r\n",
    "stds_baseline = []\r\n",
    "scores_local_full = []\r\n",
    "for j in range(5):\r\n",
    "    coeffs_global = []\r\n",
    "    coeffs_local = []\r\n",
    "    coeffs_baseline = []\r\n",
    "    # LOCAL METHOD\r\n",
    "    isotropic_representations = cluster_based(np.asarray(words), n_cluster, n_pc, emb_len)\r\n",
    "    sentence_rep = mean_pooling(isotropic_representations, sentences)\r\n",
    "    score_full = similarity(sentence_rep, emb_len)\r\n",
    "    scores_local_full.append(sper_corrcoef(dfd[\"corr\"].tolist(), score_full))\r\n",
    "    # Run k bootstrap samples to estimate error \r\n",
    "    for k in range(40):\r\n",
    "        ixs = random.choices(list(range(len(sentences)//2)),k=len(sentences))\r\n",
    "        baseline_bootstrap = []\r\n",
    "        global_bootstrap = []\r\n",
    "        local_bootstrap = []\r\n",
    "        scores_bootstrap = []\r\n",
    "        for i in ixs:\r\n",
    "            if j == 0:\r\n",
    "                baseline_bootstrap.append(baseline_sentence_rep[2*i])\r\n",
    "                global_bootstrap.append(global_sentence_rep[2*i])\r\n",
    "                baseline_bootstrap.append(baseline_sentence_rep[2*i+1])\r\n",
    "                global_bootstrap.append(global_sentence_rep[2*i+1])\r\n",
    "            local_bootstrap.append(sentence_rep[2*i])\r\n",
    "            local_bootstrap.append(sentence_rep[2*i+1])\r\n",
    "            scores_bootstrap.append(dfd['corr'].iloc[i])\r\n",
    "\r\n",
    "        if j == 0:\r\n",
    "            global_scoore_bs = similarity(global_bootstrap, emb_len)\r\n",
    "            baseline_scoore_bs = similarity(baseline_bootstrap, emb_len)\r\n",
    "        local_scoore_bs = similarity(local_bootstrap, emb_len)\r\n",
    "\r\n",
    "        if j == 0:\r\n",
    "            coeffs_baseline.append(sper_corrcoef(scores_bootstrap, baseline_scoore_bs))\r\n",
    "            coeffs_global.append(sper_corrcoef(scores_bootstrap, global_scoore_bs))\r\n",
    "        coeffs_local.append(sper_corrcoef(scores_bootstrap, local_scoore_bs))\r\n",
    "\r\n",
    "\r\n",
    "    all_scores_local += coeffs_local\r\n",
    "    if j == 0:\r\n",
    "        means_baseline.append(np.mean(coeffs_baseline))\r\n",
    "        means_global.append(np.mean(coeffs_global))\r\n",
    "        stds_global.append(np.std(coeffs_global))\r\n",
    "        stds_baseline.append(np.std(coeffs_baseline))\r\n",
    "\r\n",
    "    means_local.append(np.mean(coeffs_local))\r\n",
    "    stds_local.append(np.std(coeffs_local))\r\n",
    "    \r\n",
    "    print(\"run \",j,\", means and errors:\")\r\n",
    "    if j == 0:\r\n",
    "        print(\"baseline:\",np.mean(coeffs_baseline),\"+-\",np.std(coeffs_baseline))\r\n",
    "        print(\"global:\",np.mean(coeffs_global),\"+-\",np.std(coeffs_global))\r\n",
    "    print(\"local:\",np.mean(coeffs_local),\"+-\",np.std(coeffs_local))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.65948633320005 +- 1.0979745194922623\n",
      "71.9418373610387 +- 1.271562268471421\n",
      "71.78731463664705 +- 0.9968744583662337\n",
      "71.4765840956824 +- 1.1268121289716495\n",
      "71.819495553112 +- 1.1794197221874692\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(scores_local_full)):\r\n",
    "    print(scores_local_full[i],\"+-\",stds_local[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline bs std: +- 1.4412689453310223\n",
      "global bs std: +- 1.1140048742865518\n"
     ]
    }
   ],
   "source": [
    "print(\"baseline bs std: +-\",stds_baseline[0])\r\n",
    "print(\"global bs std: +-\",stds_global[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_local_full-mean: 71.73694359593603\n",
      "scores_local_full-std: 0.15818989808247108\n",
      "mean_scores_local_bs-std: 0.3714824887731119\n",
      "all_scores_local_bs-std: 1.1972393232938126\n"
     ]
    }
   ],
   "source": [
    "print(\"scores_local_full-mean:\", np.mean(scores_local_full))\r\n",
    "print(\"scores_local_full-std:\", np.std(scores_local_full))\r\n",
    "print(\"mean_scores_local_bs-std:\", np.std(means_local))\r\n",
    "print(\"all_scores_local_bs-std:\", np.std(all_scores_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation baseline:  48.11715446651223\n",
      "Spearman Correlation global:  64.38035591129334\n",
      "Spearman Correlation local:  67.63985006690713\n",
      "Isotropy baseline:  0.0001104029659049792\n",
      "Isotropy global:  0.5206524086147406\n",
      "Isotropy local:  0.7535314129175708\n"
     ]
    }
   ],
   "source": [
    "# performance\r\n",
    "print(\"Spearman Correlation baseline: \",sper_corrcoef(dfd['corr'], baseline_score))\r\n",
    "print(\"Spearman Correlation global: \",sper_corrcoef(dfd['corr'], global_score))\r\n",
    "print(\"Spearman Correlation local: \",sper_corrcoef(dfd['corr'], local_score))\r\n",
    "\r\n",
    "# isotropy of space\r\n",
    "print(\"Isotropy baseline: \", isotropy(np.asarray(baseline_sentence_rep, dtype=np.float64)))\r\n",
    "print(\"Isotropy global: \", isotropy(global_representations))\r\n",
    "print(\"Isotropy local: \", isotropy(isotropic_representations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation local:  65.07141874721928\n"
     ]
    }
   ],
   "source": [
    "# REPEAT LAST METHOD\r\n",
    "isotropic_representations = cluster_based(np.asarray(words), n_cluster, n_pc, emb_len)\r\n",
    "sentence_rep = mean_pooling(isotropic_representations, sentences)\r\n",
    "local_score = similarity(sentence_rep, emb_len)\r\n",
    "\r\n",
    "print(\"Spearman Correlation local: \",sper_corrcoef(dfd['corr'], local_score))\r\n",
    "#print(\"Isotropy local: \", isotropy(isotropic_representations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run multiple experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npynames = [\"roberta-stsb-test.npy\", \"roberta-sts2015-test.npy\", \"roberta-sts2014-test.npy\", \"roberta-sts2013-test.npy\", \"roberta-sts2012-test.npy\" ]\r\n",
    "# datasets = [ \"sts-test.csv\", \"../data/sts-2015-test/STS.input.ALL.csv\", \"../data/sts-2014-test/STS.input.ALL.csv\", \"../data/sts-2013-test/STS.input.ALL.csv\", \"../data/sts-2012-test/STS.input.ALL.csv\"]\r\n",
    "# appendfile = open(\"results3.txt\", \"a\")\r\n",
    "\r\n",
    "# for i in range(len(npynames)):\r\n",
    "#     dsName = datasets[i]\r\n",
    "#     dfd = pd.read_csv(dsName, delimiter='\\t' , error_bad_lines=False)\r\n",
    "#     dfd = dfd[dfd[\"corr\"] != -1]\r\n",
    "#     dfd = dfd[np.logical_not(pd.isna(dfd[\"sentence2\"]))]\r\n",
    "#     sentences = np.load(npynames[i], allow_pickle=True).tolist()\r\n",
    "#     words = getWords(sentences)\r\n",
    "\r\n",
    "#     baseline_sentence_rep = mean_pooling(words, sentences)\r\n",
    "#     baseline_score = similarity(baseline_sentence_rep, emb_len)\r\n",
    "\r\n",
    "#     # GLOBAL METHOD\r\n",
    "#     global_representations = global_method(np.asarray(words), n_pc_global, emb_len)\r\n",
    "#     global_sentence_rep = mean_pooling(global_representations, sentences)\r\n",
    "#     global_score = similarity(global_sentence_rep, emb_len)\r\n",
    "\r\n",
    "#     # LOCAL METHOD\r\n",
    "#     isotropic_representations = cluster_based(np.asarray(words), n_cluster, n_pc, emb_len)\r\n",
    "#     # calculating sentence representations based on mean word (isotropised) representations\r\n",
    "#     sentence_rep = mean_pooling(isotropic_representations, sentences)\r\n",
    "#     local_score = similarity(sentence_rep, emb_len)\r\n",
    "\r\n",
    "#     appendfile.write(npynames[i]+\"\\n\")\r\n",
    "#     appendfile.write(\"Spearman Correlation baseline: \" + str(sper_corrcoef(dfd['corr'], baseline_score)) + \"\\n\")\r\n",
    "#     appendfile.write(\"Spearman Correlation global: \" + str(sper_corrcoef(dfd['corr'], global_score))+ \"\\n\")\r\n",
    "#     appendfile.write(\"Spearman Correlation local: \" + str(sper_corrcoef(dfd['corr'], local_score))+ \"\\n\")\r\n",
    "#     appendfile.flush()\r\n",
    "#     # isotropy of space\r\n",
    "#     appendfile.write(\"Isotropy baseline: \" + str(isotropy(np.asarray(baseline_sentence_rep, dtype=np.float64)))+ \"\\n\")\r\n",
    "#     appendfile.write(\"Isotropy global: \" + str(isotropy(global_representations))+ \"\\n\")\r\n",
    "#     appendfile.write(\"Isotropy local: \" + str(isotropy(isotropic_representations))+ \"\\n\")\r\n",
    "#     appendfile.write(\"---------\\n\")\r\n",
    "#     appendfile.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit",
   "name": "python3811jvsc74a57bd0556d0f6bb3e19b5350a50c9b037347830b2ef0c6b7dbb2b89c6068584bda62c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}