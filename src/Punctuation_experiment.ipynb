{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\r\n",
    "from functions import *\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import *\r\n",
    "from transformers.tokenization_utils import TextInputPair\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "import tensorflow as tf\r\n",
    "import pickle\r\n",
    "import scipy as sc\r\n",
    "import math as mt\r\n",
    "from joblib import dump, load\r\n",
    "import random\r\n",
    "from sklearn.neighbors import KNeighborsTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\r\n",
    "casing = \"bert-base-uncased\" \r\n",
    "tokenizer = BertTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "\r\n",
    "config = BertConfig(dropout=0.2, attention_dropout=0.2 ) #hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2\r\n",
    "config.output_hidden_states = False # if true outputs all layers\r\n",
    "\r\n",
    "model = TFBertModel.from_pretrained(casing, config = config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# BERT\r\n",
    "n_cluster = 27 # Number of clusters to use\r\n",
    "n_pc = 12 # Number of main principal components to drop for local method\r\n",
    "n_pc_global = 15 # Number of main principal components to drop for global method\r\n",
    "spectkn = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2\r\n",
    "casing = \"gpt2\" \r\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = GPT2Config()\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFGPT2Model.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# GPT2\r\n",
    "n_cluster = 10\r\n",
    "n_pc = 30\r\n",
    "#n_pc = 12\r\n",
    "n_pc_global = 30\r\n",
    "spectkn = \"Ġ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa\r\n",
    "casing = \"roberta-base\"\r\n",
    "tokenizer = RobertaTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = RobertaConfig.from_pretrained(casing)\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFRobertaModel.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# RoBERTa\r\n",
    "n_cluster = 27\r\n",
    "n_pc = 12\r\n",
    "n_pc_global = 25\r\n",
    "spectkn = \"Ġ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.150k.pickle', 'rb') as f:\r\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather representations of each stop word / punctuation of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_grps = [y for y in x if \"the\" in y]\r\n",
    "of_grps = [y for y in x if \"of\" in y]\r\n",
    "period_grps =  [y for y in x if \".\" in y]\r\n",
    "comma_grps = [y for y in x if \",\" in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\r\n",
    "data_the = random.sample(the_grps,200)\r\n",
    "data_of = random.sample(of_grps,200)\r\n",
    "data_period = random.sample(period_grps,200)\r\n",
    "data_comma = random.sample(comma_grps,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comma_sents = []\r\n",
    "for i in range(200):\r\n",
    "    sents_grp = []\r\n",
    "    for j in range(6):\r\n",
    "        sents_grp.append(tokenizer.convert_ids_to_tokens(tokenizer.encode(\" \".join(data_comma[i][j])[:-2] + \".\" )))\r\n",
    "    data_comma_sents.append(sents_grp)\r\n",
    "data_comma = data_comma_sents\r\n",
    "\r\n",
    "data_period_sents = []\r\n",
    "for i in range(200):\r\n",
    "    sents_grp = []\r\n",
    "    for j in range(6):\r\n",
    "        sents_grp.append(tokenizer.convert_ids_to_tokens(tokenizer.encode(\" \".join(data_period[i][j])[:-2] + \".\" )))\r\n",
    "    data_period_sents.append(sents_grp)\r\n",
    "data_period = data_period_sents\r\n",
    "\r\n",
    "data_of_sents = []\r\n",
    "for i in range(200):\r\n",
    "    sents_grp = []\r\n",
    "    for j in range(6):\r\n",
    "        sents_grp.append(tokenizer.convert_ids_to_tokens(tokenizer.encode(\" \".join(data_of[i][j])[:-2] + \".\")))\r\n",
    "    data_of_sents.append(sents_grp)\r\n",
    "data_of = data_of_sents\r\n",
    "\r\n",
    "data_the_sents = []\r\n",
    "for i in range(200):\r\n",
    "    sents_grp = []\r\n",
    "    for j in range(6):\r\n",
    "        sents_grp.append(tokenizer.convert_ids_to_tokens(tokenizer.encode(\" \".join(data_the[i][j])[:-2] + \".\")))\r\n",
    "    data_the_sents.append(sents_grp)\r\n",
    "data_the = data_the_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_the = []\r\n",
    "for group in data_the:\r\n",
    "    gids = []\r\n",
    "    for i in range(6):\r\n",
    "        gids.append(tokenizer.convert_tokens_to_ids(group[i]))\r\n",
    "    ids_the.append(gids)\r\n",
    "\r\n",
    "ids_of = []\r\n",
    "for group in data_of:\r\n",
    "    gids = []\r\n",
    "    for i in range(6):\r\n",
    "        gids.append(tokenizer.convert_tokens_to_ids(group[i]))\r\n",
    "    ids_of.append(gids)\r\n",
    "\r\n",
    "ids_period = []\r\n",
    "for group in data_period:\r\n",
    "    gids = []\r\n",
    "    for i in range(6):\r\n",
    "        gids.append(tokenizer.convert_tokens_to_ids(group[i]))\r\n",
    "    ids_period.append(gids)\r\n",
    "\r\n",
    "ids_comma = []\r\n",
    "for group in data_comma:\r\n",
    "    gids = []\r\n",
    "    for i in range(6):\r\n",
    "        gids.append(tokenizer.convert_tokens_to_ids(group[i]))\r\n",
    "    ids_comma.append(gids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beni\\anaconda3\\envs\\repro\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "reps_comma = []\r\n",
    "for i in range(200):\r\n",
    "    groupoutput = []\r\n",
    "    for j in range(6):\r\n",
    "        groupoutput.append(model(np.asarray([ids_comma[i][j]], dtype=\"int32\"))[0][0])\r\n",
    "    reps_comma.append(np.asarray(groupoutput))\r\n",
    "\r\n",
    "reps_period = []\r\n",
    "for i in range(200):\r\n",
    "    groupoutput = []\r\n",
    "    for j in range(6):\r\n",
    "        groupoutput.append(model(np.asarray([ids_period[i][j]], dtype=\"int32\"))[0][0])\r\n",
    "    reps_period.append(np.asarray(groupoutput))\r\n",
    "\r\n",
    "reps_of = []\r\n",
    "for i in range(200):\r\n",
    "    groupoutput = []\r\n",
    "    for j in range(6):\r\n",
    "        groupoutput.append(model(np.asarray([ids_of[i][j]], dtype=\"int32\"))[0][0])\r\n",
    "    reps_of.append(np.asarray(groupoutput))\r\n",
    "\r\n",
    "reps_the = []\r\n",
    "for i in range(200):\r\n",
    "    groupoutput = []\r\n",
    "    for j in range(6):\r\n",
    "        groupoutput.append(model(np.asarray([ids_the[i][j]], dtype=\"int32\"))[0][0])\r\n",
    "    reps_the.append(np.asarray(groupoutput))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the stop word / punctuation of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \",\" #  \"the\" , \"of\" \",\" , \".\"\r\n",
    "data_ = data_comma # data_the, data_of, data_comma, data_period\r\n",
    "reps = reps_comma # reps_the, reps_of, reps_comma, reps_period\r\n",
    "spectkn_ = spectkn\r\n",
    "if token == \".\" and casing == \"roberta-base\":\r\n",
    "    spectkn_ = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results for punctuation of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenpositions = [] # list of tuple (group,sent,index,fullidx,grpix)\r\n",
    "finalreps = []\r\n",
    "tokenreps = []\r\n",
    "cnt = 0\r\n",
    "kix = 0\r\n",
    "for i in range(200):\r\n",
    "    for j in range(6):\r\n",
    "        first_added = False\r\n",
    "        for l in range(len(data_[i][j])):\r\n",
    "            finalreps.append(reps[i][j][l])\r\n",
    "            if data_[i][j][l] == spectkn_ + token and not first_added:\r\n",
    "                tokenreps.append(reps[i][j][l])\r\n",
    "                tokenpositions.append( (i,j,l,cnt,kix) )\r\n",
    "                first_added = True\r\n",
    "                kix +=1\r\n",
    "            cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<1200x1200 sparse matrix of type '<class 'numpy.float32'>'\n\twith 8400 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsTransformer(n_neighbors=6)\r\n",
    "knn.fit_transform(tokenreps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullixs = set([pos[4] for pos in tokenpositions])\r\n",
    "trueneighbors = []\r\n",
    "for pos in tokenpositions:\r\n",
    "    group,sent,index,fullidx,tokenidx = pos\r\n",
    "    scores, neighs = knn.kneighbors([tokenreps[tokenidx]], 6)\r\n",
    "    truegroupixs = set(map(lambda y: y[4], filter(lambda x: x[0] == group,tokenpositions)))\r\n",
    "    trueneighbors.append(len(set(neighs[0]).intersection(truegroupixs))/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8266666666666667"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline result\r\n",
    "np.mean(trueneighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dominant directions and repeat\r\n",
    "iso_tokenreps = cluster_based(np.array(tokenreps),1,n_pc,emb_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<1200x1200 sparse matrix of type '<class 'numpy.float64'>'\n\twith 8400 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsTransformer(n_neighbors=6)\r\n",
    "knn.fit_transform(iso_tokenreps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullixs = set([pos[4] for pos in tokenpositions])\r\n",
    "trueneighbors = []\r\n",
    "for pos in tokenpositions:\r\n",
    "    group,sent,index,fullidx,tokenidx = pos\r\n",
    "    scores, neighs = knn.kneighbors([iso_tokenreps[tokenidx]], 6)\r\n",
    "    truegroupixs = set(map(lambda y: y[4], filter(lambda x: x[0] == group,tokenpositions)))\r\n",
    "    trueneighbors.append(len(set(neighs[0]).intersection(truegroupixs))/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7934722222222222"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trueneighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('repro': conda)",
   "name": "python3811jvsc74a57bd0556d0f6bb3e19b5350a50c9b037347830b2ef0c6b7dbb2b89c6068584bda62c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}