{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import semcor\r\n",
    "from IPython.display import clear_output\r\n",
    "from functions import *\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from transformers import *\r\n",
    "from transformers.tokenization_utils import TextInputPair\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "import tensorflow as tf\r\n",
    "import pickle\r\n",
    "import scipy as sc\r\n",
    "import math as mt\r\n",
    "from joblib import dump, load\r\n",
    "from sklearn.neighbors import KNeighborsTransformer\r\n",
    "from collections import defaultdict\r\n",
    "from scipy.spatial.distance import pdist, cdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\r\n",
    "casing = \"bert-base-uncased\" \r\n",
    "tokenizer = BertTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "\r\n",
    "config = BertConfig(dropout=0.2, attention_dropout=0.2 ) #hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2\r\n",
    "config.output_hidden_states = False # if true outputs all layers\r\n",
    "\r\n",
    "model = TFBertModel.from_pretrained(casing, config = config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# BERT\r\n",
    "n_cluster = 27 # Number of clusters to use\r\n",
    "n_pc = 12 # Number of main principal components to drop for local method\r\n",
    "n_pc_global = 15 # Number of main principal components to drop for global method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2\r\n",
    "casing = \"gpt2\" \r\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = GPT2Config()\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFGPT2Model.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# GPT2\r\n",
    "n_cluster = 10\r\n",
    "n_pc = 30\r\n",
    "n_pc_global = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa\r\n",
    "casing = \"roberta-base\"\r\n",
    "tokenizer = RobertaTokenizer.from_pretrained(casing, do_lower_case=True, add_special_tokens=True)\r\n",
    "config = RobertaConfig.from_pretrained(casing)\r\n",
    "config.output_hidden_states = True\r\n",
    "\r\n",
    "model = TFRobertaModel.from_pretrained(casing, config=config)\r\n",
    "model.trainable = False\r\n",
    "emb_len = 768\r\n",
    "clear_output()\r\n",
    "\r\n",
    "# RoBERTa\r\n",
    "n_cluster = 27\r\n",
    "n_pc = 12\r\n",
    "n_pc_global = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"semcor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = semcor.tagged_sents(tag=\"sem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [] # sentix, wordix, label, word\r\n",
    "sentix = 0\r\n",
    "for sent in sents:\r\n",
    "    wordix = 0\r\n",
    "    for word in sent:\r\n",
    "        if type(word) == nltk.tree.Tree:\r\n",
    "            if word.label() and type(word.label()) == nltk.corpus.reader.wordnet.Lemma:\r\n",
    "                if word.label().synset() and word.label().synset().pos() == \"v\":\r\n",
    "                    verbs.append((sentix,wordix,word.label(),word[0]))\r\n",
    "        wordix+=1\r\n",
    "    sentix+=1\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueverbs = [\r\n",
    "    'say', 'said', 'have', 'had', 'win', 'won', 'study', 'studied', 'find', 'found', 'hold', 'held', 'make', 'made', 'tell', 'told', 'seek', 'sought', 'see', 'saw', 'get', 'got', 'shoot', 'shot', 'go', 'went', 'lead', 'led', 'leave', 'left', 'deny', 'denied', 'send', 'sent', 'keep', 'kept', 'lose', 'lost', 'feel', 'felt', 'spend', 'spent', 'draw', 'drew', 'throw', 'threw', 'try', 'tried', 'pay', 'paid', 'break', 'broke', 'come', 'came', 'run', 'ran', 'think', 'thought', 'carry', 'carried', 'catch', 'caught', 'lie', 'lay', 'fall', 'fell', 'write', 'wrote', 'know', 'knew', 'stand', 'stood', 'teach', 'taught', 'fight', 'fought', 'rise', 'rose', 'speak', 'spoken', 'choose', 'chosen', 'forget', 'forgotten', 'strike', 'struck', 'meet', 'met', 'build', 'built', 'apply', 'applied', 'sit', 'sat', 'sell', 'sold', 'buy', 'bought', 'feed', 'fed', 'ride', 'rode', 'drive', 'drove', 'wear', 'wore'\r\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_verbs = list(filter(lambda x: x[3] in trueverbs,verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_to_parse = set()\r\n",
    "for vb in filtered_verbs:\r\n",
    "    sents_to_parse = sents_to_parse.union([vb[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "11838"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents_to_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(sorted(sents_to_parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate actual index -> filtered index\r\n",
    "translation_dict = dict(zip(idxs,range(len(idxs))))\r\n",
    "translation_dict_inv = dict(zip(range(len(idxs)),idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsents = []\r\n",
    "i = 0\r\n",
    "for sent in semcor.sents():\r\n",
    "    if i in idxs:\r\n",
    "        finalsents.append(sent)\r\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase\r\n",
    "for i in range(len(finalsents)):\r\n",
    "    for j in range(len(finalsents[i])):\r\n",
    "        finalsents[i][j] = finalsents[i][j].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize\r\n",
    "ids = []\r\n",
    "for sent in finalsents:\r\n",
    "    ids.append(tokenizer.convert_tokens_to_ids(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send through model...\r\n",
    "reps = []\r\n",
    "# for i in range(len(ids)):\r\n",
    "#     reps.append(model(np.asarray([ids[i]], dtype=\"int32\"))[0])\r\n",
    "with open('gpt2_tense.pkl', 'rb') as f:\r\n",
    "    reps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('gpt2_tense.pkl', 'wb') as f:\r\n",
    "#     pickle.dump(reps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = [] # will keep dicts of verbs in order of trueverbs, {synset: [rep]}\r\n",
    "#for each verb, (for present and past form), collect their CWRs from the model outputs, store them in dicts\r\n",
    "for i in range(len(trueverbs)//2):\r\n",
    "    present_verb = trueverbs[i*2]\r\n",
    "    past_verb = trueverbs[i*2+1]\r\n",
    "    present_dict = defaultdict(list)\r\n",
    "    past_dict = defaultdict(list)\r\n",
    "    # go over all filtered verbs, fetch their CWRs... \r\n",
    "    for j in range(len(filtered_verbs)):\r\n",
    "        # present\r\n",
    "        if filtered_verbs[j][3] == present_verb:\r\n",
    "            sent_ix = translation_dict[filtered_verbs[j][0]]\r\n",
    "            wrd_ix = filtered_verbs[j][1]\r\n",
    "            present_dict[filtered_verbs[j][2].synset().name()].append(reps[sent_ix][0][wrd_ix].numpy())\r\n",
    "        # past\r\n",
    "        elif filtered_verbs[j][3] == past_verb:\r\n",
    "            sent_ix = translation_dict[filtered_verbs[j][0]]\r\n",
    "            wrd_ix = filtered_verbs[j][1]\r\n",
    "            past_dict[filtered_verbs[j][2].synset().name()].append(reps[sent_ix][0][wrd_ix].numpy())\r\n",
    "    \r\n",
    "    list_of_dicts.append(present_dict)\r\n",
    "    list_of_dicts.append(past_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions\r\n",
    "\r\n",
    "def st_sm(list_of_dicts):\r\n",
    "    \"\"\"\r\n",
    "        From list of dicts, get average distance between verb representations of same tense and same meaning \r\n",
    "    \"\"\"\r\n",
    "    # go over all verbs\r\n",
    "    means = []\r\n",
    "    for i in range(len(list_of_dicts)//2):\r\n",
    "        present_verb_dict = list_of_dicts[2*i]\r\n",
    "        past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "        # go over all of this verbs present tense synsets\r\n",
    "        for synset in present_verb_dict.keys():\r\n",
    "            verb_representations = present_verb_dict[synset]\r\n",
    "            if len(verb_representations) > 1:\r\n",
    "                means+= list(pdist(np.array(verb_representations), 'euclidean'))\r\n",
    "\r\n",
    "        # go over all of this verbs past tense synsets\r\n",
    "        for synset in past_verb_dict.keys():\r\n",
    "            verb_representations = past_verb_dict[synset]\r\n",
    "            if len(verb_representations) > 1:\r\n",
    "                means+= list(pdist(np.array(verb_representations), 'euclidean'))\r\n",
    "                #means.append(pdist(np.array(verb_representations), 'euclidean').mean())\r\n",
    "\r\n",
    "    return np.mean(means)\r\n",
    "\r\n",
    "def st_dm(list_of_dicts):\r\n",
    "    \"\"\"\r\n",
    "        From list of dicts, get average distance between verb representations of same tense and different meaning \r\n",
    "    \"\"\"\r\n",
    "    # go over all verbs\r\n",
    "    means = []\r\n",
    "    for i in range(len(list_of_dicts)//2):\r\n",
    "        present_verb_dict = list_of_dicts[2*i]\r\n",
    "        past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "        # go over all of this verbs present tense synsets\r\n",
    "        for synset in present_verb_dict.keys():\r\n",
    "            verb_representations = present_verb_dict[synset]\r\n",
    "            # get all other synsets represenattions \r\n",
    "            other_synsets = [x for x in present_verb_dict.keys() if x!= synset]\r\n",
    "            other_syn_reps = []\r\n",
    "            for ss in other_synsets:\r\n",
    "                other_syn_reps+= present_verb_dict[ss]\r\n",
    "            other_syn_reps = np.array(other_syn_reps)\r\n",
    "\r\n",
    "            \r\n",
    "            if len(other_syn_reps.shape) == 2 and len(verb_representations) > 0:\r\n",
    "                means.append(cdist(np.array(verb_representations), other_syn_reps, 'euclidean'))\r\n",
    "            \r\n",
    "\r\n",
    "        # go over all of this verbs past tense synsets\r\n",
    "        for synset in past_verb_dict.keys():\r\n",
    "            verb_representations = past_verb_dict[synset]\r\n",
    "            # get all other synsets represenattions \r\n",
    "            other_synsets = [x for x in past_verb_dict.keys() if x!= synset]\r\n",
    "            other_syn_reps = []\r\n",
    "            for ss in other_synsets:\r\n",
    "                other_syn_reps+= past_verb_dict[ss]\r\n",
    "            other_syn_reps = np.array(other_syn_reps)\r\n",
    "\r\n",
    "            if len(other_syn_reps.shape) == 2 and len(verb_representations) > 0:\r\n",
    "                means.append(cdist(np.array(verb_representations), other_syn_reps, 'euclidean'))\r\n",
    "                #means+= list(pdist(np.array(verb_representations), 'euclidean'))\r\n",
    "\r\n",
    "    finalels = []\r\n",
    "    for arr in means:\r\n",
    "        finalels += arr.flatten().tolist()\r\n",
    "    return np.mean(finalels)\r\n",
    "\r\n",
    "def dt_sm(list_of_dicts):\r\n",
    "    \"\"\"\r\n",
    "        From list of dicts, get average distance between verb representations of different tense and same meaning \r\n",
    "    \"\"\"\r\n",
    "    # go over all verbs\r\n",
    "    means = []\r\n",
    "    for i in range(len(list_of_dicts)//2):\r\n",
    "        present_verb_dict = list_of_dicts[2*i]\r\n",
    "        past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "        # look at intersection of synsets\r\n",
    "        synsets_intersect = list(set(present_verb_dict.keys()).intersection(past_verb_dict.keys()))\r\n",
    "\r\n",
    "        # go over all of this verbs filtered synsets\r\n",
    "        for synset in synsets_intersect:\r\n",
    "            verb_representations_pres = present_verb_dict[synset]\r\n",
    "            verb_representation_past = past_verb_dict[synset]\r\n",
    "            \r\n",
    "            if  len(verb_representation_past) > 0 and len(verb_representations_pres) > 0:\r\n",
    "                means.append(cdist(np.array(verb_representations_pres), np.array(verb_representation_past), 'euclidean'))\r\n",
    "            \r\n",
    "    finalels = []\r\n",
    "    for arr in means:\r\n",
    "        finalels += arr.flatten().tolist()\r\n",
    "    return np.mean(finalels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "6.201136590915077"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sm(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "6.390464035390845"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_dm(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7.092225340775829"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_sm(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reps = []\r\n",
    "for ix in range(len(list_of_dicts)):\r\n",
    "    for key in list_of_dicts[ix].keys():\r\n",
    "        all_reps += list_of_dicts[ix][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotropy(all_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Repeat for clustered data..\r\n",
    "new_dicts = []\r\n",
    "for i in range(len(list_of_dicts)//2): #len(list_of_dicts)//2\r\n",
    "    present_verb_dict = list_of_dicts[2*i]\r\n",
    "    past_verb_dict = list_of_dicts[2*i+1]\r\n",
    "\r\n",
    "    combined_reps = [] \r\n",
    "    for key in present_verb_dict.keys():\r\n",
    "        combined_reps += present_verb_dict[key]\r\n",
    "    for key in past_verb_dict.keys():\r\n",
    "        combined_reps += past_verb_dict[key]\r\n",
    "\r\n",
    "\r\n",
    "    cb_reps = cluster_based(np.array(combined_reps), 1, n_pc, 768)\r\n",
    "\r\n",
    "    present_dict_new = defaultdict(list)\r\n",
    "    past_dict_new = defaultdict(list)\r\n",
    "\r\n",
    "    u = 0\r\n",
    "    for key in present_verb_dict.keys():\r\n",
    "        present_dict_new[key] = []\r\n",
    "        for ix in range(len(present_verb_dict[key])):\r\n",
    "            present_dict_new[key].append(cb_reps[u])\r\n",
    "            u+=1\r\n",
    "    for key in past_verb_dict.keys():\r\n",
    "        past_dict_new[key] = []\r\n",
    "        for ix in range(len(past_verb_dict[key])):\r\n",
    "            past_dict_new[key].append(cb_reps[u])\r\n",
    "            u+=1\r\n",
    "            \r\n",
    "\r\n",
    "    new_dicts.append(present_dict_new)\r\n",
    "    new_dicts.append(past_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.102383363457763"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sm(new_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.487159470883429"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_dm(new_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.463559610829548"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_sm(new_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reps_new = []\r\n",
    "for ix in range(len(new_dicts)):\r\n",
    "    for key in new_dicts[ix].keys():\r\n",
    "        all_reps_new += new_dicts[ix][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotropy(all_reps_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('repro': conda)",
   "name": "python3811jvsc74a57bd0556d0f6bb3e19b5350a50c9b037347830b2ef0c6b7dbb2b89c6068584bda62c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}